> This repo is about a series of references which are written in AI-related conference or journal.
> I wrote below contents based on the [one](https://arxiv.org/pdf/2406.20095v1) and [the other](https://arxiv.org/pdf/2411.14042).
> I hope this repo will be helpful for readers to study how to cite paper.

> 이 저장소는 AI와 관련된 학회나 저널의 참고문헌들을 모아놓은 곳입니다.
> [Paper 1](https://arxiv.org/pdf/2406.20095v1), [Paper 2](https://arxiv.org/pdf/2411.14042)를 기반으로 아래의 내용을 작성하였습니다.
> 이 저장소가 독자들이 논문 인용 방법을 공부하는데 도움이 되길 바랍니다.

# URL

1. OpenAI. New models and developer products announced at devday, 2023.
<ins>URL https://openai.com/blog/new-models-and-developer-products-announced-at-devday</ins>

2. OpenAI. Hello gpt-4o: We’re announcing gpt-4o, our new flagship model that can reason
across audio, vision, and text in real time., 2024. <ins>URL https://openai.com/index/
hello-gpt-4o/.</ins>
   
3. Meta AI. Llama 3, 2024. <ins>URL https://llama.meta.com/llama3/. Accessed: 2024-06-24.</ins>

4. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality, <ins>March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/.</ins>

# arXiv

1. Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, and Daniel Cohen-Or. Cross-image
attention for zero-shot appearance transfer. <ins>arXiv preprint arXiv:2311.03335, 2023.</ins>

2. OpenAI. GPT-4 technical report. https://arxiv.org/abs/2303.08774, 2023.
: Authors, Title, arXiv, Year.

### Publications with the same 1st author(not the only one)

Focus on year citation format (i.e., 2023a, 2023b, and 2023c).

2-1. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, **2023a**.

2-2. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,
Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian
Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl
Pertsch, Jornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Pannag Sanketi,
Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent
Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and
Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale. Robotics science
and systems (RSS), **2023b**.

2-3. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho,
Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do as i can, not as i say: Grounding
language in robotic affordances. In Conference on Robot Learning (CoRL), pp. 287–318. **PMLR**,
**2023c**.

: PMLR is one of publisher

3. Same last name, but different first name

3-1. **Jiaxing Huang**, Jingyi Zhang, Kai Jiang, Han Qiu, and Shijian Lu. Visual instruction tuning towards
general-purpose multimodal model: A survey. arXiv preprint arXiv:2312.16602, **2023a**.

3-2. **Wenlong Huang**, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer:
Composable 3d value maps for robotic manipulation with language models. arXiv preprint
arXiv:2307.05973, **2023b**.

# Conference

## CVPR

Gwangbin Bae and Andrew J. Davison. Rethinking inductive biases for surface normal estimation.
In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.

: Authors, Title, CVPR, Year.

Mu Cai, Haotian Liu, Dennis Park, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, and
Yong Jae Lee. Vip-llava: Making large multimodal models understand arbitrary visual prompts. **In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)**,
2024.

: Authors, Title, Proceedings_full_name (CVPR), Year.

## ICCV

Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by
context prediction. **In Proceedings of the International Conference on Computer Vision (ICCV)**,
pp. 1422–1430, 2015.

## NeurIPS

Ricky T. Q. Chen, Brandon Amos, and Maximilian Nickel. 2021. Neural spatio-temporal point processes. <ins>In International Conference on Learning Representations</ins>

## ICML

Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li FeiFei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. **In Proceedings of the International Conference on Machine Learning (ICML)**, 2023.

## ICRA

Lerrel Pinto and Abhinav Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. **In IEEE International Conference on Robotics and Automation (ICRA), pp.3406–3413. IEEE**, 2016.

## ECCV

Jinghuan Shang, Kumara Kahatapitiya, Xiang Li, and Michael S. Ryoo. Starformer: Transformer with state-action-reward representations for visual reinforcement learning. **In Proceedings of the European Conference on Computer Vision (ECCV), pp. 462–479. Springer**, 2022a.

## AAAI

Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving sample efficiency in model-free reinforcement learning from images. **In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 10674–10681**, 2021.

## ICLR

Ricky T. Q. Chen, Brandon Amos, and Maximilian Nickel. 2021. Neural spatio-temporal point processes. <ins>In International Conference on Learning Representations.</ins>

## TACL

Bhuwan Dhingra, Jeremy R Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W Cohen. 2022. Timeaware language models as temporal knowledge bases. <ins>Transactions of the Association for Computational Linguistics, 10:257–273</ins>

## EMNLP

Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen White, and Sujay Jauhar. 2023. Making large language models better data creators. <ins>In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</ins>, pages 15349–15360.

## ACL

Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2023. Unnatural instructions: Tuning language models with (almost) no human labor. <ins>In The 61st Annual Meeting Of The Association For Computational Linguistics</ins>.

# Journal

## TMLR

Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,
Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom
Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,
Oriol Vinyals, Mahyar Bordbar, and Freitas de Nando. A generalist agent. **In Trans. on Machine
Learning Research**, 2022.

## TPAMI

Jinghuan Shang, Xiang Li, Kumara Kahatapitiya, Yu-Cheol Lee, and Michael S. Ryoo. Starformer:
Transformer with state-action-reward representations for robot learning. **IEEE Transactions on
Pattern Analysis and Machine Intelligence, pp. 1–16**, 2022b.


